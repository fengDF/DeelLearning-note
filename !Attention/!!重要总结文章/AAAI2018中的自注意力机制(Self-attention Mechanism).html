<!DOCTYPE html>
<!-- saved from url=(0037)http://ir.dlut.edu.cn/news/detail/486 -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta name="viewport" content="width=device-width">
    
    <title>大连理工大学信息检索研究室（DUTIR）-搜人搜物搜信息，重情重义重认知</title>
        <meta name="keywords" content="自注意力，self attention">

        <meta name="description" content="大连理工大学信息检索实验室主要从事信息检索、搜索引擎、文本挖掘、自然语言处理、机器学习、生物信息学方向研究。主要研究课题：查询理解与重构、排序学习、计算广告、跨语言检索、情感分析、观点挖掘、汉语情感语义资源建设、舆情分析、博客和微博的倾向性分析、地理搜索、移动搜索、音乐检索、社区发现、趋势分析、信息推荐、信息抽取、生物医学的文本挖掘。欢迎企业界和学术界的各位同仁光临指导。">
    <link rel="stylesheet" type="text/css" href="./AAAI2018中的自注意力机制(Self-attention Mechanism)_files/layout.css">
    <link rel="shortcut icon" href="http://ir.dlut.edu.cn/favicon.ico">
    <script src="./AAAI2018中的自注意力机制(Self-attention Mechanism)_files/hm.js.下载"></script><script src="./AAAI2018中的自注意力机制(Self-attention Mechanism)_files/jquery-1.11.3.min.js.下载"></script>
    <script>
        var _hmt = _hmt || [];
        (function () {
            var hm = document.createElement("script");
            hm.src = "//hm.baidu.com/hm.js?f919bc54b29c1e2113a26e3b66102efc";
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(hm, s);
        })();
    </script>

<style type="text/css">@font-face {
  font-family: 'rbicon';
  src: url(chrome-extension://dipiagiiohfljcicegpgffpbnjmgjcnf/fonts/rbicon.woff2) format("woff2");
  font-weight: normal;
  font-style: normal; }
</style></head>
<body class="body">
    <div class="wrapper">
        <div class="minornav">
            <div class="left">
                <ul>
                    <li>
                        <a href="http://ir.dlut.edu.cn/news/detail/">
                            <img src="./AAAI2018中的自注意力机制(Self-attention Mechanism)_files/home.png" alt="Home" width="15px" height="13px" border="0px">
                        </a>
                    </li>
                    <li>
                        <a href="http://ir.dlut.edu.cn/news/detail/486#">
                            <img src="./AAAI2018中的自注意力机制(Self-attention Mechanism)_files/search.png" alt="Search" width="15px" height="13px" border="0px">
                        </a>
                    </li>
                    <li>
                        <a href="mailto:irlab@dlut.edu.cn">
                            <img src="./AAAI2018中的自注意力机制(Self-attention Mechanism)_files/mail.png" alt="Mail" width="15px" height="13px" border="0px">
                        </a>
                    </li>
                </ul>
            </div>
            <div class="right">
                <ul>
                    <li style="float: right;"><a href="http://ir.dlut.edu.cn/OnlineJudge" target="_blank">DUTIR在线评测平台</a></li>
                    <li style="float: right;"><a href="http://ir.dlut.edu.cn:8100/" target="_blank">论坛</a></li>
                    <li style="float: right;"><a href="http://ir.dlut.edu.cn:8000/" target="_blank">Wiki资料平台</a></li>
                </ul>
            </div>
            <div class="clear">
            </div>
        </div>
        <!------- minornav -------------->
        <div class="banner">
        </div>
        <div class="mainnav">
            <ul>
                <li><a href="http://ir.dlut.edu.cn/">首页</a></li>
                <li><a href="http://ir.dlut.edu.cn/Introduction">学术研究</a></li>
                <li><a href="http://ir.dlut.edu.cn/Member">成员介绍</a></li>
                <li><a href="http://ir.dlut.edu.cn/News/List/1">新闻动态</a></li>
                <li><a href="http://ir.dlut.edu.cn/Project">科研项目</a></li>
                <li><a href="http://ir.dlut.edu.cn/News/List/9">学术报告</a></li>
                <li><a href="http://ir.dlut.edu.cn/Live">多彩生活</a></li>
                <li><a href="http://ir.dlut.edu.cn/Album/List">缤纷相册</a></li>
                <li><a href="http://ir.dlut.edu.cn/Evaluation">学术评测</a></li>
            </ul>
            <div class="clear">
            </div>
        </div>

        <!-------mainnav-------------->
        <div class="content">
            

<div class="left">
    <div class="title">
        研究方向
    </div>
    <ul>
        <li><a href="http://ir.dlut.edu.cn/group/detail/2">搜索引擎与自然语言处理</a></li>
        <li><a href="http://ir.dlut.edu.cn/group/detail/3">文本挖掘与机器学习</a></li>
        <li><a href="http://ir.dlut.edu.cn/group/detail/4">情感分析与观点挖掘</a></li>
        <li><a href="http://ir.dlut.edu.cn/group/detail/5">面向生物医学领域的文本挖掘</a></li>
    </ul>

    <div class="title">
        学术报告
    </div>
    <ul>
            <li>
                <a href="http://ir.dlut.edu.cn/news/detail/528" target="_blank" title="王鑫雷 The APVA-TURBO Approach to Question Answering in Knowledge Base">
                    <span>10-14</span>王鑫雷 The APVA-TURBO Approach to Question Answering in Knowledge Base
                </a>
            </li>
            <li>
                <a href="http://ir.dlut.edu.cn/news/detail/493" target="_blank" title="李楠 Generalizing Biomedical Relation Classification with Neural Adversarial Domain Adaptation">
                    <span>05-20</span>李楠 Generalizing Biomedical Relation Classification with Neural Adversarial Domain Adaptation
                </a>
            </li>
            <li>
                <a href="http://ir.dlut.edu.cn/news/detail/489" target="_blank" title="李青青 Deep learning for drug-drug interaction extraction">
                    <span>04-23</span>李青青 Deep learning for drug-drug interaction extraction
                </a>
            </li>
            <li>
                <a href="http://ir.dlut.edu.cn/news/detail/486" target="_blank" title="刁宇峰 AAAI2018中的自注意力机制(Self-attention Mechanism)">
                    <span>04-15</span>刁宇峰 AAAI2018中的自注意力机制(Self-attention Mechanism)
                </a>
            </li>
            <li>
                <a href="http://ir.dlut.edu.cn/news/detail/487" target="_blank" title="王治政 Label informed Attributed Network Embedding">
                    <span>04-15</span>王治政 Label informed Attributed Network Embedding
                </a>
            </li>
            <li>
                <a href="http://ir.dlut.edu.cn/news/detail/485" target="_blank" title="罗凌 自然语言处理中的自注意力机制（Self-attention Mechanism）">
                    <span>03-24</span>罗凌 自然语言处理中的自注意力机制（Self-attention Mechanism）
                </a>
            </li>

    </ul>
    <div class="title">
        资源下载
    </div>
    <ul>
        <li><a href="http://ir.dlut.edu.cn/EmotionOntologyDownload" target="_blank">情感本体库<img src="./AAAI2018中的自注意力机制(Self-attention Mechanism)_files/news.gif" alt="新发布"></a></li>
        <li><a href="http://ir.dlut.edu.cn/File/Download?cid=1">学习资料</a></li>
        <li><a href="http://ir.dlut.edu.cn/Graduate">学术论文</a></li>
        <li><a href="http://ir.dlut.edu.cn/File/Download?cid=2">软件工具</a></li>
        <li><a href="http://ir.dlut.edu.cn/File/Download?cid=3">其他下载</a></li>
    </ul>
</div>

<div id="RightContent">

    
<div class="right">
    <div class="article">

        <div class="title">
            刁宇峰 AAAI2018中的自注意力机制(Self-attention Mechanism)
        </div>
        <div class="description">
            新闻来源：IR实验室 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 发布时间：2018/4/15 12:56:50
        </div>
        <div class="content">
            <p style="text-align:center;text-indent:40px"><br></p><p style="text-indent:28px"><span style="font-family:宋体">近年来，注意力（</span>Attention<span style="font-family:宋体">）机制被广泛应用到基于深度学习的自然语言处理</span>(NLP)<span style="font-family:宋体">各个任务中。随着注意力机制的深入研究，各式各样的</span>attention<span style="font-family:宋体">被研究者们提出，如单个、多个、交互式等等。去年</span>6<span style="font-family:宋体">月，</span>google<span style="font-family:宋体">机器翻译团队在</span>arXiv<span style="font-family:宋体">上的《</span>Attention is all you need<span style="font-family:宋体">》论文受到了大家广泛关注，其中，他们提出的自注意力（</span>self-attention<span style="font-family:宋体">）机制和多头（</span>multi-head<span style="font-family:宋体">）机制也开始成为神经网络</span>attention<span style="font-family:宋体">的研究热点，在各个任务上也取得了不错的效果。在</span>AAAI2018<span style="font-family:宋体">的接收论文中，有</span>30<span style="font-family:宋体">余篇都使用了</span>attention<span style="font-family:宋体">机制，其中有</span>3<span style="font-family:宋体">篇使用到了</span>self-attention<span style="font-family:宋体">。本人就这篇论文中的</span>self-attention<span style="font-family:宋体">以及一些相关工作进行了学习总结（其中也参考借鉴了张俊林博士的博客“深度学习中的注意力机制</span>(2017<span style="font-family:宋体">版</span>)<span style="font-family:宋体">”和苏剑林的“《</span>Attention is All You Need<span style="font-family:宋体">》浅读（简介</span>+<span style="font-family:宋体">代码）”），和大家一起分享。</span></p><p><strong><span style="font-size:19px;font-family:宋体">一、引言</span></strong></p><p>&nbsp;&nbsp; Attention<span style="font-family:宋体">机制由视觉图像领域提出来，在</span>2014<span style="font-family:宋体">年，</span>Bahdanau<span style="font-family:宋体">在《</span>Neural Machine Translation by Jointly Learning to Align and Translate<span style="font-family:宋体">》上将其应用到机器翻译任务上，这是第一个应用到</span>NLP<span style="font-family:宋体">领域的论文。之后，</span>15<span style="font-family:宋体">、</span>16<span style="font-family:宋体">、</span>17<span style="font-family:宋体">乃至今年，都有各式各样的</span>attention<span style="font-family:宋体">机制结合深度学习网络模型被用于处理各种</span>NLP<span style="font-family:宋体">的任务。在</span>2017<span style="font-family:宋体">年，</span>google<span style="font-family:宋体">机器翻译团队发表的《</span>Attention is all you need<span style="font-family:宋体">》中大量使用了自注意力机制（</span>self-attention<span style="font-family:宋体">）来学习文本表示，脱离传统的</span>RNN/CNN<span style="font-family:宋体">，同时也使用了新颖的</span>multi-head<span style="font-family:宋体">机制。自注意力机制也成为了大家近期研究的热点，可以应用到各种</span>NLP<span style="font-family:宋体">任务上。</span></p><p>&nbsp; <span style="font-family:宋体">对于传统的机器翻译，我们可以使用</span>sequence to sequence<span style="font-family:宋体">（</span>encoder-decoder<span style="font-family:宋体">）模型来进行翻译，如下图所示。</span></p><p style="text-align:center"><img src="./AAAI2018中的自注意力机制(Self-attention Mechanism)_files/6365939388525054273511028.jpg" title="clip_image002.jpg"></p><p>&nbsp;&nbsp;&nbsp; <span style="font-family:宋体">这里，我们可以把上图抽象出来得到下图。输入序列</span>{x1, x2, x3, x4}<span style="font-family:宋体">，传入编码器（</span>encoder<span style="font-family:宋体">）中进行编码，得到语义编码</span>c<span style="font-family:宋体">，然后通过解码器（</span>decoder<span style="font-family:宋体">）进行解码，得到输出序列</span>{y1, y2, y3}<span style="font-family:宋体">，输入与输出的个数可以不相等。</span></p><p style="text-align:center"><img src="./AAAI2018中的自注意力机制(Self-attention Mechanism)_files/6365939388532854281253598.jpg" title="clip_image004.jpg"></p><p style="text-align:left">&nbsp;&nbsp; <span style="font-family:宋体">但是，这种方式会有一个问题：对于长句子的翻译会造成一定的困难，而</span>attention<span style="font-family:宋体">机制的引入可以解决这个问题。如下图所示：</span></p><p style="text-align:center"><img src="./AAAI2018中的自注意力机制(Self-attention Mechanism)_files/6365939388540654297996169.jpg" title="clip_image006.jpg"></p><p style="text-align:left;text-indent:28px"><span style="font-family:宋体">这里，我们可以看到，</span>decoder<span style="font-family:宋体">中有几个输出序列，对应的语义编码</span>c<span style="font-family:宋体">则有相同的数量，即一个语义编码</span>c<sub>i</sub><span style="font-family:宋体">对应一个输出</span>y<sub>i</sub><span style="font-family:宋体">。而每个</span>c<sub>i</sub><span style="font-family:宋体">就是由</span>attention<span style="font-family:宋体">机制得到，具体公式如下：</span></p><p style="text-indent:28px"><img src="./AAAI2018中的自注意力机制(Self-attention Mechanism)_files/6365939388550014318967984.jpg" title="clip_image008.jpg"></p><p style="text-indent:28px"><span style="font-family:宋体">回顾了传统的</span>attention<span style="font-family:宋体">模型之后，我们看一下</span>google<span style="font-family:宋体">翻译团队对</span>attention<span style="font-family:宋体">模型的高度抽取概况。他们将其映射为一个</span>query<span style="font-family:宋体">和一系列</span>&lt;key, value&gt;<span style="font-family:宋体">，最终得到输出</span>attention value<span style="font-family:宋体">的过程。这里的</span>query<span style="font-family:宋体">相当于</span>decoder<span style="font-family:宋体">中的</span>s<sub>i-1</sub><span style="font-family:宋体">，</span>key<span style="font-family:宋体">与</span>value<span style="font-family:宋体">都来自于</span>encoder<span style="font-family:宋体">的</span>h<sub>j</sub><span style="font-family:宋体">，区别在于前后状态的</span>h<sub>j</sub><span style="font-family:宋体">。然后计算</span>query<span style="font-family:宋体">与</span>key<sub>i</sub><span style="font-family:宋体">的相似度，并与</span>value<sub>i</sub><span style="font-family:宋体">进行相乘，然后求和。</span></p><p style="text-indent:28px"><img src="./AAAI2018中的自注意力机制(Self-attention Mechanism)_files/6365939388560934338467218.jpg" title="clip_image010.jpg"></p><p style="text-indent:28px"><span style="font-family:宋体">上面提到的</span>query<span style="font-family:宋体">与</span>key<span style="font-family:宋体">之间计算相似度有许多方法，如</span>dot<span style="font-family:宋体">、</span>general<span style="font-family:宋体">、</span>concat<span style="font-family:宋体">和</span>MLP<span style="font-family:宋体">等方式，具体公式如下所示。而</span>attention<span style="font-family:宋体">模型抽象为</span>query<span style="font-family:宋体">、</span>key<span style="font-family:宋体">和</span>value<span style="font-family:宋体">之间的相似度计算，总共有</span>3<span style="font-family:宋体">个阶段。第一阶段：</span>query<span style="font-family:宋体">与</span>key<sub>i</sub><span style="font-family:宋体">使用特定的相似度函数计算相似度，得到</span>s<sub>i</sub><span style="font-family:宋体">；第二阶段：对</span>s<sub>i</sub><span style="font-family:宋体">进行</span>softmax()<span style="font-family:宋体">归一化得到</span>a<sub>i</sub><span style="font-family:宋体">；第三阶段，将</span>a<sub>i</sub><span style="font-family:宋体">与</span>value<sub>i</sub><span style="font-family:宋体">对应相乘再求和，得到最终的</span>attention value<span style="font-family:宋体">。其实对比传统的</span>attention<span style="font-family:宋体">公式，我们可以看出，这两套公式还是很像的。</span></p><p style="text-indent:28px"><img src="./AAAI2018中的自注意力机制(Self-attention Mechanism)_files/6365939392357981008571663.jpg" title="clip_image012.jpg"></p><p style="margin-left:0;text-indent:0"><strong><span style="font-size:19px">二、</span></strong><strong><span style="font-size:19px;font-family:宋体">自注意力机制</span></strong></p><p style="text-indent:37px"><span style=";font-family:宋体">下面主要介绍《</span>Attention is all you need<span style=";font-family:宋体">》这篇论文，发表在</span>NIPS2017<span style=";font-family:宋体">上。这篇论文的创新性在于：（</span>1<span style=";font-family:宋体">）不同于以往基于</span>RNN<span style=";font-family:宋体">的</span>seq2seq<span style=";font-family:宋体">模型框架，该论文使用了</span>attention<span style=";font-family:宋体">机制代替了</span>RNN<span style=";font-family:宋体">搭建模型；（</span>2<span style=";font-family:宋体">）提出了多头注意力机制（</span>multi-head self-attention<span style=";font-family:宋体">）；（</span>3<span style=";font-family:宋体">）在</span>WMT2014<span style=";font-family:宋体">语料中，取得了先进结果，并且训练速度要快很多。</span></p><p style="text-indent:37px"><span style=";font-family:宋体">该模型的架构如图所示，依然符合</span>seq2seq<span style=";font-family:宋体">的架构，由</span>encoder<span style=";font-family:宋体">和</span>decoder<span style=";font-family:宋体">组成。在编码器中由许多重复的网络块组成，一个网络块由一个多头</span>attention<span style=";font-family:宋体">层和一个前向神经网络组成（而非单独使用</span>attention<span style=";font-family:宋体">模型），整个编码器栈式搭建了</span>N<span style=";font-family:宋体">个块。</span>Decoder<span style=";font-family:宋体">与</span>encoder<span style=";font-family:宋体">类似，除了编码器到解码器的学习外，还有解码器到解码器的学习。同时，为了能够更深层次的搭建网络结构，该模型使用了残差结构（</span>Add<span style=";font-family:宋体">）和对层的规范化（</span>Norm<span style=";font-family:宋体">）。</span></p><p style="text-indent:37px">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <img src="./AAAI2018中的自注意力机制(Self-attention Mechanism)_files/6365939389759016431066421.jpg" title="clip_image014.jpg"></p><p style="text-indent:37px"><span style="font-family:宋体">本文对基本的</span>attention<span style="font-family:宋体">模型进行了少许改进，提出了缩放点积</span>attention<span style="font-family:宋体">（</span>scaled dot-Product attention<span style="font-family:宋体">）。在使用点积运算进行相似度计算的基础上，缩小了</span><img src="./AAAI2018中的自注意力机制(Self-attention Mechanism)_files/6365939389766816457808992.jpg" title="clip_image016.jpg"><span style="font-family:宋体">倍（</span>d<sub>k</sub><span style="font-family:宋体">为词向量的维度）。其目的在于调节的作用，使得内积不易过大。</span></p><p style="text-indent:37px"><img src="./AAAI2018中的自注意力机制(Self-attention Mechanism)_files/6365939389776176468780808.jpg" title="clip_image018.jpg"></p><p style="text-indent:28px"><span style="font-family:宋体">多头</span>attention<span style="font-family:宋体">（</span>Multi-head attention<span style="font-family:宋体">）的结构贺公式如图所示。首先，需要对</span>query<span style="font-family:宋体">、</span>key<span style="font-family:宋体">和</span>value<span style="font-family:宋体">进行一个线性变换；然后输入到缩放点积</span>attention<span style="font-family:宋体">机制，重复做</span>h<span style="font-family:宋体">次，每次的输入为线性变换后的原始输入，这里，多头就是指做多次</span>attention<span style="font-family:宋体">之后进行拼接，每一次算一个头，每次</span>Q<span style="font-family:宋体">、</span>K<span style="font-family:宋体">和</span>V<span style="font-family:宋体">的线性变换参数</span>W<span style="font-family:宋体">是不一样的；最后，将拼接后的模型做一次线性变换，得到的值为多头</span>attention<span style="font-family:宋体">的结果。可以看出，多头</span>attention<span style="font-family:宋体">与传统的</span>attention<span style="font-family:宋体">区别在于计算了</span>h<span style="font-family:宋体">次，这样可以从不同的维度和表示子空间里学习到相关的信息，可通过</span>attention<span style="font-family:宋体">可视化机制来验证。</span></p><p style="text-indent:28px"><img src="./AAAI2018中的自注意力机制(Self-attention Mechanism)_files/6365939389783976486523378.jpg" title="clip_image020.jpg"></p><p style="text-indent:28px"><span style="font-family:宋体">在整个模型中，从编码器到解码器的地方中使用了多头</span>attention<span style="font-family:宋体">进行连接，</span>K<span style="font-family:宋体">、</span>V<span style="font-family:宋体">和</span>Q<span style="font-family:宋体">分别是编码器的层输出（这里</span>K=V<span style="font-family:宋体">）和解码器中多头</span>attention<span style="font-family:宋体">的输入，这其实跟主流的机器翻译模型中的</span>attention<span style="font-family:宋体">一样，进行传统的翻译对齐任务。然后，在编码器和解码器中都使用了多头自注意力</span>self-attention<span style="font-family:宋体">来学习文本的表示，</span>K=V=Q<span style="font-family:宋体">，即里面的每个词都要和该句子中的所有词进行</span>attention<span style="font-family:宋体">计算，其主要目的是学习句子内部的词依赖关系，捕获句子中的内部结构。</span></p><p style="text-align:center;text-indent:28px"><img src="./AAAI2018中的自注意力机制(Self-attention Mechanism)_files/6365939389793336492793366.jpg" title="clip_image022.jpg"></p><p style="text-indent:28px"><span style="font-family:宋体">这里，要着重说一下位置编码。因为该模型没有使用</span>RNN<span style="font-family:宋体">等序列模型，不能考虑到时序信息，因此，这里拟合了一个位置编码函数，来模拟词语的顺序。实验结果表明，这种方式是合理有效的。</span></p><p style="text-align:center;text-indent:28px"><img src="./AAAI2018中的自注意力机制(Self-attention Mechanism)_files/6365939389801136519535937.jpg" title="clip_image024.jpg"></p><p style="margin-left:0;text-indent:0"><strong><span style="font-size:19px">三、</span></strong><strong><span style="font-size:19px">AAAI2018</span></strong><strong><span style="font-size:19px;font-family:宋体">中的</span></strong><strong><span style="font-size:19px">self-attention</span></strong></p><p><strong><span style="font-size:19px">&nbsp;&nbsp; </span></strong><span style="font-family:宋体">在新放出来的</span>AAAI2018<span style="font-family:宋体">的论文中，共计有</span>30<span style="font-family:宋体">余篇使用</span>attention<span style="font-family:宋体">模型处理各种</span>NLP<span style="font-family:宋体">任务，这里，主要介绍使用</span>self-attention<span style="font-family:宋体">机制的三篇论文。</span></p><p style="margin-left:0;text-indent:0">1、DiSAN: Directional Self-Attention Network for RNN/CNN-Free Language Understanding</p><p>&nbsp;&nbsp; <span style="font-family:宋体">这篇论文是悉尼科技大学</span>UTS<span style="font-family:宋体">的张成奇教授发表的论文，发表在</span>AAAI2018<span style="font-family:宋体">上。该论文旨在提出一种通用框架，在自然语言推理（</span>natural language inference<span style="font-family:宋体">）、情感分析、语义关系（</span>semantic relatedness<span style="font-family:宋体">）、句子分类（</span>sentence classifications<span style="font-family:宋体">）等任务中均取得较好的效果。</span></p><p><span style="font-family:宋体">创新点主要有两点：第一，多维度：这里的</span>attention<span style="font-family:宋体">被用于计算每个特征上；第二，方向性：使用一个或多个位置</span>mask<span style="font-family:宋体">对</span>attention<span style="font-family:宋体">进行建模。</span></p><p style="text-align:center"><img src="./AAAI2018中的自注意力机制(Self-attention Mechanism)_files/6365939389808936527278507.jpg" title="clip_image026.jpg"></p><p>&nbsp; <span style="font-family:宋体">这里，输入序列为</span>x = [x<sub>1</sub>, x<sub>2</sub>, ..., x<sub>n</sub>]<span style="font-family:宋体">，词向量维度为</span>d<sub>e</sub><span style="font-family:宋体">。</span></p><p>&nbsp; <span style="font-family:宋体">首先，对</span>embedding<span style="font-family:宋体">层经过全连接层，得到</span>hidden state<span style="font-family:宋体">：</span>h = [h<sub>1</sub>, h<sub>2</sub>, ..., h<sub>n</sub>]<span style="font-family:宋体">，公式如下：</span></p><p style="text-align:center"><img src="./AAAI2018中的自注意力机制(Self-attention Mechanism)_files/6365939392365781016314234.jpg" title="clip_image028.jpg"></p><p>&nbsp; <span style="font-family:宋体">然后，计算</span>h<sub>i</sub><span style="font-family:宋体">与</span>h<sub>j</sub><span style="font-family:宋体">之间的相似度函数</span>f(h<sub>i</sub>, h<sub>j</sub>)<span style="font-family:宋体">，公式如下所示。其中</span>c=5<span style="font-family:宋体">。</span></p><p style="text-align:center"><img src="./AAAI2018中的自注意力机制(Self-attention Mechanism)_files/6365939392373581034056804.jpg" title="clip_image030.jpg"></p><p style="text-indent:14px"><span style="font-family:宋体">公式里的</span>M<span style="font-family:宋体">为位置编码</span>mask<span style="font-family:宋体">。本文共提出三种位置</span>mask<span style="font-family:宋体">方式，分别为前向（</span>fw<span style="font-family:宋体">）、后向（</span>bw<span style="font-family:宋体">）和对角（</span>diag<span style="font-family:宋体">），公式如下：</span></p><p style="text-align:center;text-indent:14px"><img src="./AAAI2018中的自注意力机制(Self-attention Mechanism)_files/6365939392382941049326792.jpg" title="clip_image032.jpg">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <img src="./AAAI2018中的自注意力机制(Self-attention Mechanism)_files/6365939392390741067069363.jpg" title="clip_image034.jpg"></p><p style="text-align:left;text-indent:14px">&nbsp;<span style="font-family:宋体">然后，对得到的相似度函数</span>f(h<sub>i</sub>, h<sub>j</sub>)<span style="font-family:宋体">进行</span>softmax<span style="font-family:宋体">归一化操作，得到</span>(0, 1)<span style="font-family:宋体">之间的数</span>P<span style="font-family:宋体">。接着，对应与输入向量</span>x<span style="font-family:宋体">进行相乘，求和，最终得到输出向量</span>s<span style="font-family:宋体">。</span></p><p style="text-align:left;text-indent:14px">&nbsp;<span style="font-family:宋体">同时，本文引入门机制。对向量</span>s<span style="font-family:宋体">和隐层状态</span>h<span style="font-family:宋体">进行线性变换后再求和，并进行</span>sigmoid<span style="font-family:宋体">操作，得到</span>F<span style="font-family:宋体">。用门</span>F<span style="font-family:宋体">控制</span>h<span style="font-family:宋体">和</span>s<span style="font-family:宋体">的比例，得到最终的输出向量</span>u<span style="font-family:宋体">。</span></p><p style="text-align:center;text-indent:14px"><img src="./AAAI2018中的自注意力机制(Self-attention Mechanism)_files/6365939392398541074811934.jpg" title="clip_image036.jpg"></p><p style="text-align:left;text-indent:28px"><span style="font-family:宋体">本文，在输入向量上，分别使用前向自注意力机制和后向自注意力机制，将两部分的结果进行拼接，并使用多维度的</span>self-attention<span style="font-family:宋体">，最后输出。实验表明，该模型在很多任务中均取得了很好的结果。</span></p><p style="text-align:center;text-indent:14px"><img src="./AAAI2018中的自注意力机制(Self-attention Mechanism)_files/6365939393900823718371389.jpg" title="clip_image038.jpg"></p><p style="margin-left:0;text-indent:0">2、Deep Semantic Role Labeling with Self-Attention</p><p>&nbsp;&nbsp; <span style="font-family:宋体">这篇论文来自</span>AAAI2018<span style="font-family:宋体">，厦门大学的工作。将</span>self-attention<span style="font-family:宋体">应用到了语义角色标注任务（</span>SRL<span style="font-family:宋体">）上，看作一个序列标注问题，使用</span>BIO<span style="font-family:宋体">标签进行标注。然后提出使用深度注意力网络（</span>Deep Attentional Neural Network<span style="font-family:宋体">）进行标注，网络结构如下。在每一个网络块中，有一个</span>RNN/CNN/FNN<span style="font-family:宋体">子层和一个</span>self-attention<span style="font-family:宋体">子层组成。最后直接利用</span>softmax<span style="font-family:宋体">当成标签分类进行序列标注。该论文在网络块数为</span>10<span style="font-family:宋体">的时候，取得了较好的结果。</span></p><p style="text-align:center"><img src="./AAAI2018中的自注意力机制(Self-attention Mechanism)_files/6365939393910183739343205.jpg" title="clip_image040.jpg">&nbsp;&nbsp;</p><p style="margin-left:0;text-indent:0">3、Simultaneously Self-Attending to All Mentions for Full-Abstract Biological Relation Extraction</p><p>&nbsp;&nbsp;&nbsp; <span style="font-family:宋体">这篇论文是</span>AndrewMcCallum<span style="font-family:宋体">团队应用</span>self-attention<span style="font-family:宋体">在生物医学关系抽取任务上的一个工作，应该是已经被</span>NAACL2018<span style="font-family:宋体">接收。这篇论文作者提出了一个文档级别的生物关系抽取模型，里面做了不少工作，感兴趣的读者可以更深入阅读原文。我们这里只简单提一下他们</span>self-attention<span style="font-family:宋体">的应用部分。论文模型的整体结构如下图。</span></p><p style="text-indent:28px"><img src="./AAAI2018中的自注意力机制(Self-attention Mechanism)_files/6365939393921103748842438.jpg" title="clip_image042.jpg"></p><p style="margin-left:0;text-indent:0">四、<span style="font-family:宋体">总结</span></p><p>&nbsp;&nbsp;&nbsp; Google<span style="font-family:宋体">提出的</span>self-attention<span style="font-family:宋体">是</span>attention<span style="font-family:宋体">模型的一种特殊形式，是自己学习自己的过程，</span>Q=K=V<span style="font-family:宋体">；提出的</span>multi-head attention<span style="font-family:宋体">是通过计算多次来捕获不同维度不同子空间上的相关信息。</span>Self-attention<span style="font-family:宋体">可以不考虑词与词之间的距离而直接计算依赖关系，能够学习到一个句子的内部结构，能够简单并行的计算，可以脱离</span>CNN<span style="font-family:宋体">和</span>RNN<span style="font-family:宋体">，但是需要合理的考虑和设置位置函数。当然，从</span>AAAI2018<span style="font-family:宋体">年的论文可以看出，</span>self-attention<span style="font-family:宋体">也可以当作一个层，与</span>RNN<span style="font-family:宋体">、</span>CNN<span style="font-family:宋体">和</span>FNN<span style="font-family:宋体">等配合使用，能够更好的解决</span>NLP<span style="font-family:宋体">领域的任务。</span></p><p><span style="font-family:宋体">参考文献：</span></p><p>[1] Vaswani, Ashish, et al. Attention is all you need. Advances in Neural Information Processing Systems. 2017.</p><p>[2] Shen, T.; Zhou, T.; Long, G.; Jiang, J.; Pan, S.; and Zhang, C. Disan: Directional self-attention network for rnn/cnn-free language understanding. AAAI 2018.</p><p>[3] Verga P, Strubell E, McCallum A. Simultaneously Self-Attending to All Mentions for Full-Abstract Biological Relation Extraction. AAAI 2018.</p><p>[4] Tan Z, Wang M, Xie J, et al. Deep Semantic Role Labeling with Self-Attention. AAAI 2018.</p><p><strong>&nbsp;</strong></p><p><span style="font-family:宋体">参考博客：</span></p><p><span style="font-family:宋体">张俊林，深度学习中的注意力机制</span>(2017<span style="font-family:宋体">版</span>)<span style="font-family:宋体">，</span>https://blog.csdn.net/malefactor/article/details/78767781</p><p><span style="font-family:宋体">苏剑林，《</span>Attention is All You Need<span style="font-family:宋体">》浅读（简介</span>+<span style="font-family:宋体">代码），</span>https://kexue.fm/archives/4765</p><p style="text-indent:28px">&nbsp;</p><p><br></p>
        </div>
        <!---- article ---->
    </div>
</div>


</div>
            <div class="clear">
            </div>
        </div>
        <div class="footer">
            <div class="center">
                <span>
                    <a href="http://ir.dlut.edu.cn/Manage/Index" title="进入后台管理系统" target="_self">
                        IRLab
                    </a>
                </span>
                <span>版权所有：大连理工大学信息检索研究室（DUTIR）  联系我们: irlab@dlut.edu.cn&nbsp;&nbsp;&nbsp;</span>
            </div>
        </div>
        <!-------footer-------------->
    </div>


<div id="rememberry__extension__root" style="all: unset;"></div></body></html>